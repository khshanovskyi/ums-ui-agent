# Users Management Agent

**Create Production ready Agent with Tool use pattern and MCP**

## ğŸ“‹ Requirements

- **Python**: 3.11 or higher
- **Dependencies**: Listed in `requirements.txt`
- **API Access**: DIAL API key with appropriate permissions
- **Network**: EPAM VPN connection for internal API access
- Docker and Docker Compose

### If the task is hard for you, then switch to the `main` or `with-detailed-description` branch

## Task

In this task you will need to implement an Agent with classical Tool use patten and connect to several MCP servers:
- We need to support streaming and non-streaming flow
- Will use UI chat with response streaming
- All the conversations must be stored in the Redis storage
- As tools, we will use tools from 3 different MCP Servers
    - UMS MCP: Server that works with UMS Service, we will run it locally from docker-compose `http://localhost:8005/mcp"`
    - Fetch MCP: Remote Server that has tools to fetch WEB content `https://remote.mcpservers.org/fetch/mcp`
    - DucDuckGo: Will run it locally (with application start in docker container), provides with WEB Search capabilities `mcp/duckduckgo:latest`
- Since agent is working with PII it seems that we need to handle the cases with Credit card information disclosure
- Run [docker-compose](docker-compose.yml) with UMS, UMS MCP and Redis
- Additionally, support both OpenAiI (GPT) and Anthropic models (two different clients)

## ğŸ—ï¸ Architecture

Suggestion of how it should work

```
â”œâ”€â”€ ğŸ“‚ agent/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ clients/
â”‚   â”‚   â”œâ”€â”€ openai_client.py       â„¹ï¸ Holds logic with connection to OpenAI, tool calling (heart of this Agent)
â”‚   â”‚   â”œâ”€â”€ http_mcp_client.py     â„¹ï¸ Connects to HTTP Streaming MCP servers (http://localhost:8005/mcp" and Fetch)
â”‚   â”‚   â””â”€â”€ stdio_mcp_client.py    â„¹ï¸ Connects to STDIO MCP Servers that will be run locally while client start
â”‚   â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”‚   â””â”€â”€ message.py             â„¹ï¸ Model with message in OpenAi format
â”‚   â”œâ”€â”€ app.py                     â„¹ï¸ Main start point with endpoints and management
â”‚   â”œâ”€â”€ conversation_manager.py    â„¹ï¸ Handles all the actions with Conversations (CRUD), holds AI Client and Redis client
â”‚   â””â”€â”€ prompts.py                 â„¹ï¸ Provides System promp for agent
â”œâ”€â”€ docker-compose.yml             â„¹ï¸ Set up to run UMS, UMS MCP and Redis
â””â”€â”€ index.html                     â„¹ï¸ Chat UI, should work in streaming mode
```

<img src="/flow_diagrams/general_flow.png" alt="General Flow Diagram" />

<img src="/flow_diagrams/chat-agent_communication_flow.png" alt="Communication Flow" />

<img src="/flow_diagrams/ui-chat.png" alt="UI Chat" />

## Redis insight

- You can connect to Redis Insight by URL http://localhost:6380
- To see the conversations add database with URL `redis-ums:6379`